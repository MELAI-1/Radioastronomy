{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MELAI-1/Radioastronomy/blob/main/GaussianProcess_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNvs5pNTKqqC"
      },
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SJ21ggSKqqH"
      },
      "source": [
        "# Sampling from a Gaussian process\n",
        "\n",
        "Let us assume that some field $x(t)$ follows a Gaussian process i.e.\n",
        "\n",
        "$$ x(t) \\sim \\mathcal{GP}(m(t), k(t, t')).  $$\n",
        "\n",
        "How can we draw samples of $x$ at arbitrary points in the domain $t \\in \\mathbb{R}$?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9Lmu8XBKqqM"
      },
      "outputs": [],
      "source": [
        "# choose a random domain\n",
        "n = 1000\n",
        "t = np.linspace(0, 10, n)\n",
        "\n",
        "# define mean and covariance functions\n",
        "def mean(t):\n",
        "    return np.zeros(t.shape)  # zero mean\n",
        "\n",
        "# squared exponential covariance function\n",
        "def sqexp(t, tp, sigmaf, l):\n",
        "    return sigmaf**2 * np.exp(-0.5 * (t - tp)**2 / l**2)\n",
        "\n",
        "\n",
        "# now we can construct the covariance matrix\n",
        "sigmaf = 1.0\n",
        "l = 0.1\n",
        "K = np.zeros((n, n))\n",
        "for i in range(n):\n",
        "    for j in range(n):\n",
        "        K[i, j] = sqexp(t[i], t[j], sigmaf, l)\n",
        "\n",
        "# to draw samples we compute a Cholesky decomposition of the covariance matrix\n",
        "L = np.linalg.cholesky(K + 1e-6*np.eye(n))  # note the jitter term for numerical stability\n",
        "\n",
        "# draw samples\n",
        "n_samples = 5\n",
        "f = mean(t)[:, None] + np.dot(L, np.random.randn(n, n_samples))\n",
        "\n",
        "# plot samples\n",
        "plt.figure()\n",
        "plt.plot(t, f)\n",
        "plt.xlabel('t')\n",
        "plt.ylabel('f(t)')\n",
        "plt.title('Samples from GP')\n",
        "plt.show()\n",
        "\n",
        "# 1) - try altering the hyper-parameter values and see how the samples change\n",
        "# 2) - try altering the mean function and see how the samples change\n",
        "# 3) - try altering the covariance function and see how the samples change\n",
        "# 4) - try altering the number of samples and see how the samples change\n",
        "# 5) - try altering the domain and see how the samples change"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCi2GR2EKqqP"
      },
      "source": [
        "# Gaussian process denoising\n",
        "\n",
        "Given observations (dropping $t$ for simplicity)\n",
        "\n",
        "$$ y = x + \\epsilon, ~~ \\epsilon \\sim \\mathcal{N}(0, \\Sigma)  $$\n",
        "\n",
        "we know that the likelihood for the problem takes the form\n",
        "\n",
        "$$ p(y | x) = \\frac{1}{\\sqrt{(2\\pi)^n |\\Sigma|}} \\exp\\left(-\\frac{1}{2}(y - x)^\\dagger \\Sigma^{-1} (y - x) \\right). $$\n",
        "\n",
        "We can also assume a Gaussian process prior on $x$ which, assuming a zero mean function, would take the form\n",
        "\n",
        "$$ p(x) = \\frac{1}{\\sqrt{(2\\pi)^n |K|}} \\exp\\left(-\\frac{1}{2}x^\\dagger K^{-1} x \\right). $$\n",
        "\n",
        "By Bayes' law, the posterior is given by the product of the likelihood and prior (up to a normalisation constant). We also know that this must have the form of a Gaussian so the term in the exponent of the posterior must, at least up to some constant $C$, be proportional to\n",
        "\n",
        "$$ (x - \\bar{x})^\\dagger D^{-1} (x - \\bar{x})  $$\n",
        "\n",
        "where $\\bar{x}$ is the posterior mean and $D$ is the posterior covariance. Expanding we find\n",
        "\n",
        "$$ x^\\dagger D^{-1} x - 2 x^\\dagger D^{-1} \\bar{x} + \\bar{x}^\\dagger D^{-1} \\bar{x} + C  $$\n",
        "\n",
        "where the factor of $2$ stems from the fact that $x^\\dagger D^{-1} \\bar{x} = \\bar{x}^\\dagger D^{-1} x$ since the term is a real scalar and we have included the constant $C$ for completeness. Also expanding the terms in the exponent of the (-log) likelihood times (-log) prior gives\n",
        "\n",
        "$$ (y - x)^\\dagger \\Sigma^{-1} (y - x) + x^\\dagger K^{-1} x = x^\\dagger \\left(\\Sigma^{-1} + K^{-1}\\right) x - 2 x^\\dagger \\Sigma^{-1} y + y^\\dagger \\Sigma^{-1} y $$\n",
        "\n",
        "Now we can read off the unknowns by comparing coefficients. First, from the quadratic term, we have\n",
        "\n",
        "$$ D^{-1} = \\Sigma^{-1} + K^{-1}  $$\n",
        "\n",
        "which is the sought after posterior covariance matrix. We can find its inverse in a numerically stable way using the matrix inversion lemma\n",
        "\n",
        "$$ D = \\Sigma - \\Sigma (K + \\Sigma)^{-1} \\Sigma = \\Sigma - \\Sigma K^{-1}_y \\Sigma, $$\n",
        "\n",
        "where we have labelled $K_y = K + \\Sigma$ for convenience.\n",
        "\n",
        "Next, by comparing coefficients for the linear term, we find the posterior mean as\n",
        "\n",
        "$$ D^{-1}\\bar{x} = \\Sigma^{-1} y ~~ \\rightarrow ~~  \\bar{x} = D \\Sigma^{-1} y = (I - \\Sigma K^{-1}_y) y = y - \\Sigma K_y^{-1} y.   $$\n",
        "\n",
        "So let's see how this works in practice. Let's first simulate some data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TGXEh_dKqqS"
      },
      "outputs": [],
      "source": [
        "# choose a random domain\n",
        "n = 100\n",
        "t = 10*np.sort(np.random.rand(n))  # does not have to be sorted\n",
        "\n",
        "# now we make up some complicated function that we want the Gaussian Process to model\n",
        "x_true = 0.25*t**2 + 10*np.sin(t)\n",
        "\n",
        "# add some noise\n",
        "sigma_n = 1.0\n",
        "noise = sigma_n* np.random.randn(n)\n",
        "y = x_true + noise\n",
        "\n",
        "# plot the noisy data\n",
        "plt.plot(t, x_true, 'k')\n",
        "plt.errorbar(t, y, yerr=sigma_n, fmt='xr', label='data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njC9h9uMKqqU"
      },
      "outputs": [],
      "source": [
        "# we choose some arbitrary hyper-parameters to start with\n",
        "sigmaf = 0.1\n",
        "l = 1.0\n",
        "\n",
        "# recompute the kernel on the new domain\n",
        "K = np.zeros((n, n))\n",
        "for i in range(n):\n",
        "    for j in range(n):\n",
        "        K[i, j] = sqexp(t[i], t[j], sigmaf, l)\n",
        "\n",
        "# construct the noise covariance matrix\n",
        "Sigma = np.eye(n) * sigma_n**2\n",
        "Sigma_inv = np.eye(n)/sigma_n**2  # because it is diagonal\n",
        "\n",
        "# now we can compute the posterior mean and covariance\n",
        "K_y = K + Sigma\n",
        "# this is not always numerically stable (see use of SVD below)\n",
        "xbar = y - Sigma.dot(np.linalg.solve(K_y, y))\n",
        "D = Sigma - Sigma.dot(np.linalg.solve(K_y, Sigma))\n",
        "\n",
        "# let's plot the posterior mean and covariance\n",
        "plt.plot(t, x_true, 'k', label='true function')\n",
        "plt.plot(t, xbar, 'b', label='posterior mean')\n",
        "plt.fill_between(t, xbar - np.sqrt(np.diag(D)), xbar + np.sqrt(np.diag(D)), color='b', alpha=0.2, label='posterior std')\n",
        "plt.legend()\n",
        "\n",
        "# 1) - try altering the hyper-parameter values and see how the posterior changes\n",
        "# 2) - try altering the noise level and see how the posterior changes\n",
        "# 3) - draw samples from the posterior and plot them"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZckOLYrpKqqW"
      },
      "source": [
        "# Hyper-parameter tuning\n",
        "\n",
        "Clearly the posterior depends on the values of the hyper-parameters. These hyper-parameters must be tuned by maximising the marginal likelihood. The negative log of the marginal likelihood is given (up to some constants) by\n",
        "\n",
        "$$ -\\log p(y | \\theta) = y^\\dagger K_y^{-1} y + \\log |K_y|, $$\n",
        "\n",
        "note how we have encoded the implicit dependence on the hyper-parameters $\\theta = [\\sigma_f, l, \\sigma_n]$ as a conditional distribution (strictly speaking we should also encode the dependence on the training points $t$ but we omit this for convenience). Training consists of minimising this function with respect to $\\theta$. We'll do this with one of scipy's optimisation routines.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cez3tO2JKqqW"
      },
      "outputs": [],
      "source": [
        "# since we'll be evaluating this function many times we provide a more efficient implementation\n",
        "def sqexp2(r, sigmaf, l):  # here r = |t - tp|\n",
        "    return sigmaf**2 * np.exp(-0.5 * r**2 / l**2)\n",
        "\n",
        "tdiffs = np.abs(t[:, None] - t[None, :])\n",
        "\n",
        "def marginal_likelihood(theta, y, tdiffs):\n",
        "    n = t.size\n",
        "    sigmaf = theta[0]\n",
        "    l = theta[1]\n",
        "    Ky = sqexp2(tdiffs, sigmaf, l) + np.eye(n)*theta[2]**2\n",
        "    u, s, v = np.linalg.svd(Ky, hermitian=True)\n",
        "    logdetK = np.sum(np.log(s))\n",
        "    Kyinv = u.dot(v/s.reshape(n, 1))\n",
        "    alpha = Kyinv.dot(y)\n",
        "    # Z = (np.vdot(y, alpha) + logdetK)/2\n",
        "    return np.vdot(y, alpha) + logdetK\n",
        "\n",
        "\n",
        "# choose initial guess for hyper-parameters\n",
        "theta0 = np.array([1.0, 1.0, 1.0])\n",
        "res = minimize(marginal_likelihood, theta0, args=(y, tdiffs), bounds=[(1e-3, None), (1e-3, None), (1e-3, None)])\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TX9ZX90WKqqZ"
      },
      "outputs": [],
      "source": [
        "# we recompute the posterior mean and covariance using optimized hyper-parameters\n",
        "theta = res.x\n",
        "sigmaf = theta[0]\n",
        "l = theta[1]\n",
        "Sigma = np.eye(n) * theta[2]**2\n",
        "Ky = sqexp2(tdiffs, sigmaf, l) + Sigma\n",
        "u, s, v = np.linalg.svd(Ky, hermitian=True)\n",
        "Kyinv = u.dot(v/s.reshape(n, 1))\n",
        "xbar = y - Sigma.dot(Kyinv.dot(y))\n",
        "D = Sigma - Sigma.dot(Kyinv.dot(Sigma))\n",
        "\n",
        "# let's plot the posterior mean and covariance\n",
        "plt.plot(t, x_true, 'k', label='true function')\n",
        "plt.plot(t, xbar, 'b', label='posterior mean')\n",
        "plt.fill_between(t, xbar - np.sqrt(np.diag(D)), xbar + np.sqrt(np.diag(D)), color='b', alpha=0.2, label='posterior std')\n",
        "plt.legend()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pfb",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}